# Apache-Spark-and-Scala

# Table of Content
1. Big Data
2. HADOOP ECOSYSTEM



# 1. Big Data

#### The History of Big Data

Although the concept of big data itself is relatively new, the origins of large data sets go back to the 1960s and '70s when the world of data was just getting started, with the first data centers and the development of the relational database.

Around 2005, people began to realize just how much data users generated through Facebook, YouTube, and other online services. Hadoop (an open-source framework created specifically to store and analyzes big data sets) was developed that same year. NoSQL also began to gain popularity during this time.

The development of open-source frameworks, such as Hadoop (and more recently, Spark) was essential for the growth of big data because they make big data easier to work with and cheaper to store. In the years since then, the volume of big data has skyrocketed. Users are still generating huge amounts of data—but it’s not just humans who are doing it.

With the advent of the Internet of Things (IoT) , more objects and devices are connected to the internet, gathering data on customer usage patterns and product performance. The emergence of machine learning has produced still more data.

While big data has come far, its usefulness is only just beginning. Cloud computing has expanded big data possibilities even further. The cloud offers truly elastic scalability, where developers can simply spin up ad hoc clusters to test a subset of data. 

#### Benefits of Big Data and Data Analytics:
•	Big data makes it possible for you to gain more complete answers because you have more information.
•	More complete answers mean more confidence in the data—which means a completely different approach to tackling problems.


# 2. HADOOP ECOSYSTEM

“Hadoop Ecosystem is neither a programming language nor a service; it is a platform or framework which solves big data problems.”
You can consider it as a suite which encompasses a number of services (ingesting, storing, analyzing and maintaining) inside it. Let us discuss and get a brief idea about how the services work individually and in collaboration.
Below are the Hadoop components, that together form a Hadoop ecosystem, I will be covering each of them in this blog:

•	HDFS -> Hadoop Distributed File System
•	YARN -> Yet Another Resource Negotiator
•	MapReduce -> Data processing using programming
•	Spark -> In-memory Data Processing
•	PIG, HIVE-> Data Processing Services using Query (SQL-like)
•	HBase -> NoSQL Database
•	Mahout, Spark MLlib -> Machine Learning
•	Apache Drill -> SQL on Hadoop
•	Zookeeper -> Managing Cluster
•	Oozie -> Job Scheduling
•	Flume, Sqoop -> Data Ingesting Services
•	Solr & Lucene -> Searching & Indexing 
•	Ambari -> Provision, Monitor and Maintain cluster
